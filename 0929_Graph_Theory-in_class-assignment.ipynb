{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to successfully complete this assignment you need to participate both individually and in groups during class on **Tuesday September 29**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# In-Class Assignment: Graph Theory\n",
    "<a href=\"http://sixdegrees.hu/last.fm/\"><img src=\"http://sixdegrees.hu/last.fm/images/lastfm_360_graph_white.png\"><p style=\"text-align: right;\">\n",
    "network of musicians\n",
    "</p>\n",
    "<p style=\"text-align: right;\">Interactive Image</p></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Agenda for today's class (80 minutes)\n",
    "\n",
    "\n",
    "1. [(20 minutes) Review of pre-class assignment](#Review_of_pre-class_assignment)\n",
    "1. [(20 minutes) Web Scraping](#Web_Scraping)\n",
    "1. [(20 minutes) Python Scripting](#Python_Scripting)\n",
    "1. [(20 minutes) Quick pdoc3 practice](#Quick_pdoc3_practice)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a name=\"Review_of_pre-class_assignment\"></a>\n",
    "# 1. Review of pre-class assignment\n",
    "\n",
    "* [0928-Graph_Theory-pre-class-assignment](0928-Graph_Theory-pre-class-assignment.ipynb)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a name=\"Web_Scraping\"></a>\n",
    "# 2. Web Scraping\n",
    "As a group run and review the code and answer the questions below.\n",
    "\n",
    "Note: example modified from http://www.netinstructions.com/how-to-make-a-web-crawler-in-under-50-lines-of-python-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "from html.parser import HTMLParser  \n",
    "from urllib import parse\n",
    "from urllib.request import urlopen  \n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a class called LinkParser that inherits some\n",
    "# methods from HTMLParser which is why it is passed into the definition\n",
    "class LinkParser(HTMLParser):\n",
    "    \"\"\"A website Parser to recursivly look though links in a webpage\"\"\"\n",
    "    # This is a function that HTMLParser normally has\n",
    "    # but we are adding some functionality to it\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        \"\"\"Function to look for the begining of a link. Links normally look\n",
    "        like <a href=\"www.someurl.com\"></a>\"\"\"\n",
    "        if tag == 'a':\n",
    "            for (key, value) in attrs:\n",
    "                if key == 'href':\n",
    "                    # We are grabbing the new URL. We are also adding the\n",
    "                    # base URL to it. For example:\n",
    "                    # www.netinstructions.com is the base and\n",
    "                    # somepage.html is the new URL (a relative URL)\n",
    "                    #\n",
    "                    # We combine a relative URL with the base URL to create\n",
    "                    # an absolute URL like:\n",
    "                    # www.netinstructions.com/somepage.html\n",
    "                    newUrl = parse.urljoin(self.baseUrl, value)\n",
    "                    # And add it to our colection of links:\n",
    "                    self.links = self.links + [newUrl]\n",
    "\n",
    "    def getLinks(self, url):\n",
    "        \"\"\"This is a new function that we are creating to get links\n",
    "        that our spider() function will call\"\"\"\n",
    "        self.links = []\n",
    "        # Remember the base URL which will be important when creating\n",
    "        # absolute URLs\n",
    "        self.baseUrl = url\n",
    "        # Use the urlopen function from the standard Python 3 library\n",
    "        response = urlopen(url)\n",
    "        # Make sure that we are looking at HTML and not other things that\n",
    "        # are floating around on the internet (such as\n",
    "        # JavaScript files, CSS, or .PDFs for example)\n",
    "        if 'text/html' in response.getheader('Content-Type'):\n",
    "            htmlBytes = response.read()\n",
    "            # Note that feed() handles Strings well, but not bytes\n",
    "            # (A change from Python 2.x to Python 3.x)\n",
    "            htmlString = htmlBytes.decode(\"utf-8\")\n",
    "            self.feed(htmlString)\n",
    "            return htmlString,self.links #htmlString, self.links\n",
    "        if 'text/plain' in response.getheader('Content-Type'):\n",
    "            return url,[]\n",
    "        else:\n",
    "            return \"\",[]\n",
    "\n",
    "\n",
    "def spider(url, maxPages):  \n",
    "    \"\"\"Main Spider function which takes in a URL and the number of pages \n",
    "    to search through before Stopping.\"\"\"\n",
    "    pagesToVisit = [url]\n",
    "    numberVisited = 0\n",
    "\n",
    "    # The main loop. Create a LinkParser and get all the links on the page.\n",
    "    # Also search the page for the word or string\n",
    "    # In our getLinks function we return the web page\n",
    "    # (this is useful for searching for the word)\n",
    "    # and we return a set of links from that web page\n",
    "    # (this is useful for where to go next)\n",
    "    while numberVisited < maxPages and pagesToVisit != []:\n",
    "        numberVisited = numberVisited +1\n",
    "        # Start from the beginning of our collection of pages to visit:\n",
    "        url = pagesToVisit[0]\n",
    "        pagesToVisit = pagesToVisit[1:]\n",
    "\n",
    "        try:\n",
    "            print(numberVisited, \"Visiting:\", url)\n",
    "            parser = LinkParser()\n",
    "            data, links = parser.getLinks(url)\n",
    "            pagesToVisit = pagesToVisit + links\n",
    "        except:\n",
    "            print(\" **Failed to parse page!**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G = spider('http://www.msu.edu/', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "We would like to modify the above code to generate a graph of websites visited by the web scraper.  Nodes are the website URLs and the edges are the links between the websites. We would then like to plot the adjacency list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>DO THIS:</font>**   things you would like to see in a programming module for graphs?  In other words, what kinds of things would you want to see in a graph theory library to help solve the problem above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your selection criteria here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>DO THIS:</font>** - Share your ideas with your group. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write any additional selection criteria that came up during your group discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>DO THIS:</font>** - Select someone from your group to share your group's ideas with the class.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write any additional selection criteria that came up during the class discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>DO THIS:</font>** Everyone get out their laptops and search for a programming library that will support graph theory.  Pick your favorite based on the properties and features you wanted during group discussion. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list here of the libraries you found. mark the one that you feel best matches your criteria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>DO THIS:</font>** Modify the above code to generate a graph of of links using the LinkParser from above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>DO THIS:</font>** As a group modify the above code to generate an adjacency list of websites visited by the web scraper.  Nodes are the website URLs and the edges are the links between the websites. Plot the resulting graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a name=\"Python_Scripting\"></a>\n",
    "\n",
    "# 3. Python Scripting\n",
    "\n",
    "Now lets start to move away from jupyter notebooks.  Notebooks are great for communicating but not great for making repeatable and reusable code.  \n",
    "\n",
    "&#9989; **<font color=red>DO THIS:</font>** Copy and paste the above code to a new python file (```graphweb.py```) in your current directory.  See if you can get the following code to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphweb\n",
    "\n",
    "graphweb.spider('http://www.msu.edu/', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    " Now that we have some idea about how the above code works. Think about what you would need to do to download sub-hourly temperature data for Michigan from the NOAA climate reference network:\n",
    "\n",
    "https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/us-climate-reference-network-uscrn\n",
    "\n",
    "&#9989; **<font color=red>DO THIS:</font>** In your group, see if you can figure out how to adjust the above code to scrape the aboe website and download all the temperature data for a particular observatory.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"Quick_pdoc3_practice\"></a>\n",
    "\n",
    "# 4. Quick pdoc3 practice\n",
    "\n",
    "We will be using ```pdoc3``` with our next project milestone.  Use this time in class to download and run pdoc3 on the code you generated. \n",
    "\n",
    "&#9989; **<font color=red>DO THIS:</font>** Install pdoc on your machine.  If you are using jupyterhub, use the following command in bash:\n",
    "\n",
    "\n",
    "```bash\n",
    "    pip install --user pdoc3\n",
    "```\n",
    "\n",
    "Or, if you prefer, you can use conda environments (prefered) and install using the following:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```bash\n",
    "    pip install --user pdoc3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>DO THIS:</font>** Once pdoc3 is installed they may need to add \"~/.local/bin/\" to your terminal path. In jupyterhub I recommend adding the following to your .bashrc file and restarting the terminal:\n",
    "\n",
    "```bash\n",
    "    PATH=$PATH:~/.local/bin/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>DO THIS:</font>**  Open up the docs folder and click on one of the html files to see the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>DO THIS:</font>**  For fun, try pdoc on a single python file (for example the ```graphweb.py``` file you just created.\n",
    "```bash\n",
    "   pdoc3 --force --html --output-dir ./docs graphweb.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Congratulations, we're done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Resources:\n",
    "\n",
    "\n",
    "- [Website](https://msu-cmse-courses.github.io/cmse802-f20-student/)\n",
    "- [ZOOM](https://msu.zoom.us/j/97272546850)\n",
    "- [Syllabus](https://docs.google.com/document/d/e/2PACX-1vT9Wn11y0ECI_NAUl_2NA8V5jcD8dXKJkqUSWXjlawgqr2gU5hII3IsE0S8-CPd3W4xsWIlPAg2YW7D/pub)\n",
    "- [Schedule](https://docs.google.com/spreadsheets/d/e/2PACX-1vQRAm1mqJPQs1YSLPT9_41ABtywSV2f3EWPon9szguL6wvWqWsqaIzqkuHkSk7sea8ZIcIgZmkKJvwu/pubhtml?gid=2142090757&single=true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by Dirk Colbry, Michigan State University\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
